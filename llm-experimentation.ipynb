{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837c60ec-aa3f-4102-a58f-aeb5b7d5f8f7",
   "metadata": {},
   "source": [
    "# LLM features\n",
    "Here I'll be experimenting with obtaining LLM features. First, I want to create a few simplified functions that will only take the sequence as an input, and output an array with the corresponding embeddings at the AA level. I'll try a few LLM like ESM, protBERT and protT5. I will start with ESM\n",
    "\n",
    "## ESM\n",
    "First we gotta install ESM through `pip`: `pip install esm` Note that you can run this on the worker which will install a cached version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7855062b-147b-4f97-bdb5-d9857d7c57ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://soft-proxy.scicore.unibas.ch/repository/python-all/simple\n",
      "Requirement already satisfied: fair-esm in /scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b46e4-0e36-47b8-9ba4-d7fa55fd4265",
   "metadata": {},
   "source": [
    "Next, you need to load the model. You need to have the model downloaded though, so run this on `login-transfer` first so you can download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae9dc67-64c8-49df-89d8-d94e7ad23c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ce6a4-06b9-4358-a556-e26f19693b0c",
   "metadata": {},
   "source": [
    "The data is stored as a list of tuples with the structure `[(seq_name, sequence)]`. The data then needs to be divided into batches using `batch_converter()`, and then we calculate the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94538b4c-815f-419b-a1fd-11f3f20ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d772e3-78f4-43d4-aa6f-49f4258a179c",
   "metadata": {},
   "source": [
    "Finally, the following obtains the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2eedc8-c3fa-47d0-ac57-0f487c69673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d5633-a82c-4b88-bb9e-21b32a3407d4",
   "metadata": {},
   "source": [
    "Which gives us a tensor of `proteins x max_len x embedding size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032264b5-39e8-46b6-9e68-76f2722b8516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 73, 1280])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf1cd6-ba37-4e4d-8e54-3c8c4f6958fd",
   "metadata": {},
   "source": [
    "Pretty simple, right? lets turn this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c0688c-63b3-4dd3-a15d-b69c2f715575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.4402e-02,  3.1896e-02, -2.2837e-04,  ..., -3.0202e-01,\n",
       "           1.6993e-01, -9.5368e-02],\n",
       "         [ 5.3387e-02, -1.4939e-01, -6.0084e-03,  ..., -1.4319e-01,\n",
       "           5.8913e-02,  4.5721e-02],\n",
       "         [ 5.7838e-02, -1.7693e-01,  2.1494e-01,  ..., -1.3017e-01,\n",
       "           1.6030e-01, -2.4029e-02],\n",
       "         ...,\n",
       "         [ 1.1289e-01, -1.0321e-01,  2.4431e-01,  ..., -7.9042e-02,\n",
       "           7.4088e-02,  4.6810e-02],\n",
       "         [ 1.6301e-01, -1.2352e-01,  2.2617e-01,  ..., -1.2443e-01,\n",
       "           5.8596e-02,  1.2715e-02],\n",
       "         [ 1.6387e-01, -9.8342e-02,  1.2440e-01,  ..., -2.0597e-01,\n",
       "           1.6841e-01, -1.1368e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "def get_esm_embedding(sequence: Union[str, list, tuple]) -> torch.Tensor:\n",
    "    global esm_model\n",
    "    global alphabet\n",
    "    global batch_converter\n",
    "    \n",
    "    try: \n",
    "        if esm_model:\n",
    "            print('ESM is loaded')\n",
    "    except:\n",
    "        # Load the model if its not loaded\n",
    "        esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "        esm_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    \n",
    "    # Convert input to be [(name, sequence), ...]\n",
    "    # Can be a single string, list of strings, tuple, or list of tuples.\n",
    "    if type(sequence) == str:\n",
    "        data = [('protein0',sequence)]\n",
    "    elif type(sequence) == list:\n",
    "        data = [(f'protein{x}',sequence[x]) for x in range(len(sequence))]\n",
    "    elif type(sequence) == tuple and len(sequence) == 2:\n",
    "        data = [sequence]\n",
    "    elif type(sequence[0]) == tuple and len(sequence[0]) == 2:\n",
    "        data = sequence\n",
    "        \n",
    "    # Obtain the batches\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    return token_representations\n",
    "    \n",
    "get_esm_embedding(('protein0', 'MMMMMMMMM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7e8a7-5235-460b-923b-ec9ad28347b3",
   "metadata": {},
   "source": [
    "Perfect, that works! Now lets move on to protBERT.\n",
    "\n",
    "## protBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff4f2e-1931-47ac-a4fa-d56299a4cd2d",
   "metadata": {},
   "source": [
    "Just like with esm, we first need to install the library. This time we can do it with `mamba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c143a51-0279-4816-be0c-96673852c726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "███████████████/  /██/  /██/  /██/  /████████████████████████\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        ███╗   ███╗ █████╗ ███╗   ███╗██████╗  █████╗\n",
      "        ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗\n",
      "        ██╔████╔██║███████║██╔████╔██║██████╔╝███████║\n",
      "        ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║\n",
      "        ██║ ╚═╝ ██║██║  ██║██║ ╚═╝ ██║██████╔╝██║  ██║\n",
      "        ╚═╝     ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═════╝ ╚═╝  ╚═╝\n",
      "\n",
      "        mamba (1.4.1) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "█████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "Looking for: ['transformers']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "conda-forge/linux-64 \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "main/linux-64        \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "main/noarch          \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "r/linux-64           \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gmain/noarch                                                   No change\n",
      "r/linux-64                                                    No change\n",
      "[+] 0.1s\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━━   0.0 B @  ??.?MB/s Downloaded  0.1s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.1s\n",
      "main/linux-64        \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B @  ??.?MB/s             0.1s\n",
      "r/noarch             \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "bioconda/linux-64    \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "bioconda/noarch      \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "nvidia/linux-64      \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "nvidia/noarch        \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                          No change\n",
      "conda-forge/noarch                                            No change\n",
      "main/linux-64                                                 No change\n",
      "r/noarch                                                      No change\n",
      "bioconda/linux-64                                             No change\n",
      "bioconda/noarch                                               No change\n",
      "nvidia/linux-64                                               No change\n",
      "nvidia/noarch                                                 No change\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /scicore/home/schwede/goetze0000/mambaforge/envs/hackathon\n",
      "\n",
      "  All requested packages already installed\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!mamba install -c conda-forge transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb6602-0a7d-409c-9474-9e6d3cc629a9",
   "metadata": {},
   "source": [
    "The Huggingface entry gives the following code to obtain the internal representations: (note you need to download the model first on `login-transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc5e764-eadb-4ad2-92d4-44704aa7ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "sequence_Example = \"A E T C Z A O\"\n",
    "sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f28cf-cf59-4c10-84ff-31791c802559",
   "metadata": {},
   "source": [
    "This is pretty simple. It first loads the model, replaces U, Z, O and B with X and then tokenizes the sequences. Then it runs the sequence and returns the encoded input.\n",
    "\n",
    "The ouput should be `protein x max_len x embedding_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db32a213-eb23-4144-abec-d8d6f6c4a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
      "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
      "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
      "         ...,\n",
      "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
      "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
      "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0d4035-973c-41ae-818c-0423a17cfde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
       "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
       "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
       "         ...,\n",
       "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
       "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
       "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_protbert_embeddings(sequence) -> torch.Tensor: \n",
    "    \n",
    "    try:\n",
    "        if tokenizer is not None or protbert_model is not None:\n",
    "            print('protBERT loaded')\n",
    "    except:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "        protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    \n",
    "    sequence = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "    encoded_input = tokenizer(sequence, return_tensors='pt')\n",
    "    output = protbert_model(**encoded_input)\n",
    "    \n",
    "    return output.last_hidden_state\n",
    "\n",
    "get_protbert_embeddings('\"A E T C Z A A E T C Z A A E T C Z A A E T C Z A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c45cc8-19a5-4a59-bdc3-137079cb390c",
   "metadata": {},
   "source": [
    "This is dumb, I didnt realize protBERT is only for protein level embeddings. Nevertheless, I'll leave it here.\n",
    "\n",
    "Now lets move on to protT5.\n",
    "\n",
    "## ProtT5\n",
    "do this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eee08f-0aa7-4541-86aa-a8c1d2a47bf8",
   "metadata": {},
   "source": [
    "## Built-in functions from graphein\n",
    "So it turns out that graphein has built-in functions for getting ESM embeddings at the residue level, and will immediately embed this into the graph as well. Lets see how it works.\n",
    "First we have to construct a graph from some protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56c5b44f-f190-4206-8067-b41f2aa1e823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/23 12:32:22] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule                                         <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">embeddings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         graphein.protein.features.sequence.embeddings, you need to install:   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec cannot be installed via conda                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Alternatively, you can install graphein with the extras:              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pip install graphein<span style=\"font-weight: bold\">[</span>extras<span style=\"font-weight: bold\">]</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/23 12:32:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule                                         \u001b]8;id=878338;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\u001b\\\u001b[2membeddings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=475215;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         graphein.protein.features.sequence.embeddings, you need to install:   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec                                                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec cannot be installed via conda                                  \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Alternatively, you can install graphein with the extras:              \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pip install graphein\u001b[1m[\u001b[0mextras\u001b[1m]\u001b[0m                                          \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/23 12:32:26] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.visualisation, you  <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">visualisation.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         need to install: pytorch3d                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pytorch3d                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/23 12:32:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.visualisation, you  \u001b]8;id=52345;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py\u001b\\\u001b[2mvisualisation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=384777;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         need to install: pytorch3d                                         \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d    \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pytorch3d                                                          \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.meshes, you need to        <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">meshes.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py#29\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         install: pytorch3d                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d pytorch3d <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.meshes, you need to        \u001b]8;id=810479;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py\u001b\\\u001b[2mmeshes.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=860617;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py#29\u001b\\\u001b[2m29\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         install: pytorch3d                                                        \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d pytorch3d \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Deprotonating protein. This removes H atoms from the pdb_df dataframe    <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#186\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Deprotonating protein. This removes H atoms from the pdb_df dataframe    \u001b]8;id=99080;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99870;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#186\u001b\\\u001b[2m186\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Detected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span> total nodes                                                 <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#438\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Detected \u001b[1;36m198\u001b[0m total nodes                                                 \u001b]8;id=811171;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=892328;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#438\u001b\\\u001b[2m438\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Reading meiler embeddings from:                                       <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">amino_acid.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#57\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">3.10/site-packages/graphein/protein/features/nodes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meiler_embeddings.</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">csv</span>                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Reading meiler embeddings from:                                       \u001b]8;id=269975;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\u001b\\\u001b[2mamino_acid.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=199759;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#57\u001b\\\u001b[2m57\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python\u001b[0m \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m3.10/site-packages/graphein/protein/features/nodes/\u001b[0m\u001b[95mmeiler_embeddings.\u001b[0m \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[95mcsv\u001b[0m                                                                   \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2aaecf8415a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from graphein.protein.graphs import construct_graph\n",
    "\n",
    "config = ProteinGraphConfig()\n",
    "g = construct_graph(config=config, path=\"../structures/1a9m.pdb\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882524-6f27-43b4-8c7f-bdd1335f69c7",
   "metadata": {},
   "source": [
    "Now to obtain the ESM embeddings I use the `graphein.protein.features.sequence.embeddings.compute_esm_embedding` function. I probably need to download it again...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a537cd-b58a-4a89-993a-5578fa8c959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scicore/home/schwede/goetze0000/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'esmfold_structure_module_only_8M' from 'esm.pretrained' (/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/esm/pretrained.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphein\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m esm_residue_embedding\n\u001b[0;32m----> 3\u001b[0m g_m \u001b[38;5;241m=\u001b[39m \u001b[43mesm_residue_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mesm1b_t33_650M_UR50S\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m g_m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py:200\u001b[0m, in \u001b[0;36mesm_residue_embedding\u001b[0;34m(G, model_name, output_layer)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mComputes ESM residue embeddings from a protein sequence and adds the to the\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mgraph.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m:rtype: nx.Graph\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chain \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mgraph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 200\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_esm_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchain\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresidue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# remove start and end tokens from per-token residue embeddings\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embedding[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py:125\u001b[0m, in \u001b[0;36mcompute_esm_embedding\u001b[0;34m(sequence, representation, model_name, output_layer)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_esm_embedding\u001b[39m(\n\u001b[1;32m     82\u001b[0m     sequence: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     83\u001b[0m     representation: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     84\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm1b_t33_650M_UR50S\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m     output_layer: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m33\u001b[39m,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Computes sequence embedding using Pre-trained ESM model from FAIR\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    :rtype: np.ndarray\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     model, alphabet \u001b[38;5;241m=\u001b[39m \u001b[43m_load_esm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     batch_converter \u001b[38;5;241m=\u001b[39m alphabet\u001b[38;5;241m.\u001b[39mget_batch_converter()\n\u001b[1;32m    128\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    129\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotein1\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequence),\n\u001b[1;32m    130\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py:78\u001b[0m, in \u001b[0;36m_load_esm_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_esm_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm1b_t33_650M_UR50S\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Loads pre-trained FAIR ESM model from torch hub.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    :return: loaded pre-trained model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebookresearch/esm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/torch/hub.py:584\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[1;32m    583\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m    587\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/hackathon/lib/python3.10/site-packages/torch/hub.py:98\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     96\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_esm_main/hubconf.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     esm1_t6_43M_UR50S,\n\u001b[1;32m     10\u001b[0m     esm1_t12_85M_UR50S,\n\u001b[1;32m     11\u001b[0m     esm1_t34_670M_UR50D,\n\u001b[1;32m     12\u001b[0m     esm1_t34_670M_UR50S,\n\u001b[1;32m     13\u001b[0m     esm1_t34_670M_UR100,\n\u001b[1;32m     14\u001b[0m     esm1b_t33_650M_UR50S,\n\u001b[1;32m     15\u001b[0m     esm1v_t33_650M_UR90S,\n\u001b[1;32m     16\u001b[0m     esm1v_t33_650M_UR90S_1,\n\u001b[1;32m     17\u001b[0m     esm1v_t33_650M_UR90S_2,\n\u001b[1;32m     18\u001b[0m     esm1v_t33_650M_UR90S_3,\n\u001b[1;32m     19\u001b[0m     esm1v_t33_650M_UR90S_4,\n\u001b[1;32m     20\u001b[0m     esm1v_t33_650M_UR90S_5,\n\u001b[1;32m     21\u001b[0m     esm2_t6_8M_UR50D,\n\u001b[1;32m     22\u001b[0m     esm2_t12_35M_UR50D,\n\u001b[1;32m     23\u001b[0m     esm2_t30_150M_UR50D,\n\u001b[1;32m     24\u001b[0m     esm2_t33_650M_UR50D,\n\u001b[1;32m     25\u001b[0m     esm2_t36_3B_UR50D,\n\u001b[1;32m     26\u001b[0m     esm2_t48_15B_UR50D,\n\u001b[1;32m     27\u001b[0m     esm_if1_gvp4_t16_142M_UR50,\n\u001b[1;32m     28\u001b[0m     esm_msa1_t12_100M_UR50S,\n\u001b[1;32m     29\u001b[0m     esm_msa1b_t12_100M_UR50S,\n\u001b[1;32m     30\u001b[0m     esmfold_structure_module_only_8M,\n\u001b[1;32m     31\u001b[0m     esmfold_structure_module_only_8M_270K,\n\u001b[1;32m     32\u001b[0m     esmfold_structure_module_only_35M,\n\u001b[1;32m     33\u001b[0m     esmfold_structure_module_only_35M_270K,\n\u001b[1;32m     34\u001b[0m     esmfold_structure_module_only_150M,\n\u001b[1;32m     35\u001b[0m     esmfold_structure_module_only_150M_270K,\n\u001b[1;32m     36\u001b[0m     esmfold_structure_module_only_650M,\n\u001b[1;32m     37\u001b[0m     esmfold_structure_module_only_650M_270K,\n\u001b[1;32m     38\u001b[0m     esmfold_structure_module_only_3B,\n\u001b[1;32m     39\u001b[0m     esmfold_structure_module_only_3B_270K,\n\u001b[1;32m     40\u001b[0m     esmfold_structure_module_only_15B,\n\u001b[1;32m     41\u001b[0m     esmfold_v0,\n\u001b[1;32m     42\u001b[0m     esmfold_v1,\n\u001b[1;32m     43\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'esmfold_structure_module_only_8M' from 'esm.pretrained' (/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/esm/pretrained.py)"
     ]
    }
   ],
   "source": [
    "from graphein.protein.features.sequence.embeddings import esm_residue_embedding\n",
    "\n",
    "g_m = esm_residue_embedding(g, model_name = 'esm1b_t33_650M_UR50S')\n",
    "\n",
    "g_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65457e21-b4ce-495b-9101-dc8a0f4376c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mambaforge-hackathon]",
   "language": "python",
   "name": "conda-env-mambaforge-hackathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
