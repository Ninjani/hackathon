{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837c60ec-aa3f-4102-a58f-aeb5b7d5f8f7",
   "metadata": {},
   "source": [
    "# LLM features\n",
    "Here I'll be experimenting with obtaining LLM features. First, I want to create a few simplified functions that will only take the sequence as an input, and output an array with the corresponding embeddings at the AA level. I'll try a few LLM like ESM, protBERT and protT5. I will start with ESM\n",
    "\n",
    "## ESM\n",
    "First we gotta install ESM through `pip`: `pip install esm` Note that you can run this on the worker which will install a cached version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7855062b-147b-4f97-bdb5-d9857d7c57ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://soft-proxy.scicore.unibas.ch/repository/python-all/simple\n",
      "Collecting fair-esm\n",
      "  Using cached https://soft-proxy.scicore.unibas.ch/repository/python-all/packages/fair-esm/2.0.0/fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
      "Installing collected packages: fair-esm\n",
      "Successfully installed fair-esm-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b46e4-0e36-47b8-9ba4-d7fa55fd4265",
   "metadata": {},
   "source": [
    "Next, you need to load the model. You need to have the model downloaded though, so run this on `login-transfer` first so you can download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ae9dc67-64c8-49df-89d8-d94e7ad23c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ce6a4-06b9-4358-a556-e26f19693b0c",
   "metadata": {},
   "source": [
    "The data is stored as a list of tuples with the structure `[(seq_name, sequence)]`. The data then needs to be divided into batches using `batch_converter()`, and then we calculate the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94538b4c-815f-419b-a1fd-11f3f20ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d772e3-78f4-43d4-aa6f-49f4258a179c",
   "metadata": {},
   "source": [
    "Finally, the following obtains the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff2eedc8-c3fa-47d0-ac57-0f487c69673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d5633-a82c-4b88-bb9e-21b32a3407d4",
   "metadata": {},
   "source": [
    "Which gives us a tensor of `proteins x max_len x embedding size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032264b5-39e8-46b6-9e68-76f2722b8516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 73, 1280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf1cd6-ba37-4e4d-8e54-3c8c4f6958fd",
   "metadata": {},
   "source": [
    "Pretty simple, right? lets turn this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c0688c-63b3-4dd3-a15d-b69c2f715575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.4402e-02,  3.1896e-02, -2.2837e-04,  ..., -3.0202e-01,\n",
       "           1.6993e-01, -9.5368e-02],\n",
       "         [ 5.3387e-02, -1.4939e-01, -6.0084e-03,  ..., -1.4319e-01,\n",
       "           5.8913e-02,  4.5721e-02],\n",
       "         [ 5.7838e-02, -1.7693e-01,  2.1494e-01,  ..., -1.3017e-01,\n",
       "           1.6030e-01, -2.4029e-02],\n",
       "         ...,\n",
       "         [ 1.1289e-01, -1.0321e-01,  2.4431e-01,  ..., -7.9042e-02,\n",
       "           7.4088e-02,  4.6810e-02],\n",
       "         [ 1.6301e-01, -1.2352e-01,  2.2617e-01,  ..., -1.2443e-01,\n",
       "           5.8596e-02,  1.2715e-02],\n",
       "         [ 1.6387e-01, -9.8342e-02,  1.2440e-01,  ..., -2.0597e-01,\n",
       "           1.6841e-01, -1.1368e-01]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "def get_esm_embedding(sequence: Union[str, list, tuple]) -> torch.Tensor:\n",
    "    \n",
    "    # Load the model\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    esm_model.eval()  # disables dropout for deterministic results\n",
    "    \n",
    "    \n",
    "    # Convert input to be [(name, sequence), ...]\n",
    "    # Can be a single string, list of strings, tuple, or list of tuples.\n",
    "    if type(sequence) == str:\n",
    "        data = [('protein0',sequence)]\n",
    "    elif type(sequence) == list:\n",
    "        data = [(f'protein{x}',sequence[x]) for x in range(len(sequence))]\n",
    "    elif type(sequence) == tuple and len(sequence) == 2:\n",
    "        data = [sequence]\n",
    "    elif type(sequence[0]) == tuple and len(sequence[0]) == 2:\n",
    "        data = sequence\n",
    "        \n",
    "    # Obtain the batches\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    return token_representations\n",
    "    \n",
    "get_esm_embedding(('protein0', 'MMMMMMMMM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7e8a7-5235-460b-923b-ec9ad28347b3",
   "metadata": {},
   "source": [
    "Perfect, that works! Now lets move on to protBERT.\n",
    "\n",
    "## protBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c8b23-4f11-4b1a-8047-f787c19561f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mambaforge-hackathon]",
   "language": "python",
   "name": "conda-env-mambaforge-hackathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
