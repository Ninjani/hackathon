{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837c60ec-aa3f-4102-a58f-aeb5b7d5f8f7",
   "metadata": {},
   "source": [
    "# LLM features\n",
    "Here I'll be experimenting with obtaining LLM features. First, I want to create a few simplified functions that will only take the sequence as an input, and output an array with the corresponding embeddings at the AA level. I'll try a few LLM like ESM, protBERT and protT5. I will start with ESM\n",
    "\n",
    "## ESM\n",
    "First we gotta install ESM through `pip`: `pip install esm` Note that you can run this on the worker which will install a cached version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7855062b-147b-4f97-bdb5-d9857d7c57ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://soft-proxy.scicore.unibas.ch/repository/python-all/simple\n",
      "Requirement already satisfied: fair-esm in /scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b46e4-0e36-47b8-9ba4-d7fa55fd4265",
   "metadata": {},
   "source": [
    "Next, you need to load the model. You need to have the model downloaded though, so run this on `login-transfer` first so you can download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae9dc67-64c8-49df-89d8-d94e7ad23c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ce6a4-06b9-4358-a556-e26f19693b0c",
   "metadata": {},
   "source": [
    "The data is stored as a list of tuples with the structure `[(seq_name, sequence)]`. The data then needs to be divided into batches using `batch_converter()`, and then we calculate the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94538b4c-815f-419b-a1fd-11f3f20ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d772e3-78f4-43d4-aa6f-49f4258a179c",
   "metadata": {},
   "source": [
    "Finally, the following obtains the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2eedc8-c3fa-47d0-ac57-0f487c69673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d5633-a82c-4b88-bb9e-21b32a3407d4",
   "metadata": {},
   "source": [
    "Which gives us a tensor of `proteins x max_len x embedding size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032264b5-39e8-46b6-9e68-76f2722b8516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 73, 1280])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf1cd6-ba37-4e4d-8e54-3c8c4f6958fd",
   "metadata": {},
   "source": [
    "Pretty simple, right? lets turn this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c0688c-63b3-4dd3-a15d-b69c2f715575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.4402e-02,  3.1896e-02, -2.2837e-04,  ..., -3.0202e-01,\n",
       "           1.6993e-01, -9.5368e-02],\n",
       "         [ 5.3387e-02, -1.4939e-01, -6.0084e-03,  ..., -1.4319e-01,\n",
       "           5.8913e-02,  4.5721e-02],\n",
       "         [ 5.7838e-02, -1.7693e-01,  2.1494e-01,  ..., -1.3017e-01,\n",
       "           1.6030e-01, -2.4029e-02],\n",
       "         ...,\n",
       "         [ 1.1289e-01, -1.0321e-01,  2.4431e-01,  ..., -7.9042e-02,\n",
       "           7.4088e-02,  4.6810e-02],\n",
       "         [ 1.6301e-01, -1.2352e-01,  2.2617e-01,  ..., -1.2443e-01,\n",
       "           5.8596e-02,  1.2715e-02],\n",
       "         [ 1.6387e-01, -9.8342e-02,  1.2440e-01,  ..., -2.0597e-01,\n",
       "           1.6841e-01, -1.1368e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "\n",
    "def get_esm_embedding(sequence: Union[str, list, tuple]) -> torch.Tensor:\n",
    "    global esm_model\n",
    "    global alphabet\n",
    "    global batch_converter\n",
    "    \n",
    "    try: \n",
    "        if esm_model:\n",
    "            print('ESM is loaded')\n",
    "    except:\n",
    "        # Load the model if its not loaded\n",
    "        esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "        esm_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    \n",
    "    # Convert input to be [(name, sequence), ...]\n",
    "    # Can be a single string, list of strings, tuple, or list of tuples.\n",
    "    if type(sequence) == str:\n",
    "        data = [('protein0',sequence)]\n",
    "    elif type(sequence) == list:\n",
    "        data = [(f'protein{x}',sequence[x]) for x in range(len(sequence))]\n",
    "    elif type(sequence) == tuple and len(sequence) == 2:\n",
    "        data = [sequence]\n",
    "    elif type(sequence[0]) == tuple and len(sequence[0]) == 2:\n",
    "        data = sequence\n",
    "        \n",
    "    # Obtain the batches\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    \n",
    "    return token_representations\n",
    "    \n",
    "get_esm_embedding(('protein0', 'MMMMMMMMM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7e8a7-5235-460b-923b-ec9ad28347b3",
   "metadata": {},
   "source": [
    "Perfect, that works! Now lets move on to protBERT.\n",
    "\n",
    "## protBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff4f2e-1931-47ac-a4fa-d56299a4cd2d",
   "metadata": {},
   "source": [
    "Just like with esm, we first need to install the library. This time we can do it with `mamba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c143a51-0279-4816-be0c-96673852c726",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "███████████████/  /██/  /██/  /██/  /████████████████████████\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        ███╗   ███╗ █████╗ ███╗   ███╗██████╗  █████╗\n",
      "        ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗\n",
      "        ██╔████╔██║███████║██╔████╔██║██████╔╝███████║\n",
      "        ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║\n",
      "        ██║ ╚═╝ ██║██║  ██║██║ ╚═╝ ██║██████╔╝██║  ██║\n",
      "        ╚═╝     ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═════╝ ╚═╝  ╚═╝\n",
      "\n",
      "        mamba (1.4.1) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "█████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "Looking for: ['transformers']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "conda-forge/linux-64 \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "main/linux-64        \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "main/noarch          \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
      "r/linux-64           \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gmain/noarch                                                   No change\n",
      "r/linux-64                                                    No change\n",
      "[+] 0.1s\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━━   0.0 B @  ??.?MB/s Downloaded  0.1s\n",
      "conda-forge/noarch   \u001b[90m━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.1s\n",
      "main/linux-64        \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B @  ??.?MB/s             0.1s\n",
      "r/noarch             \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "bioconda/linux-64    \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "bioconda/noarch      \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "nvidia/linux-64      \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\n",
      "nvidia/noarch        \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   0.0 B @  ??.?MB/s             0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                          No change\n",
      "conda-forge/noarch                                            No change\n",
      "main/linux-64                                                 No change\n",
      "r/noarch                                                      No change\n",
      "bioconda/linux-64                                             No change\n",
      "bioconda/noarch                                               No change\n",
      "nvidia/linux-64                                               No change\n",
      "nvidia/noarch                                                 No change\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /scicore/home/schwede/goetze0000/mambaforge/envs/hackathon\n",
      "\n",
      "  All requested packages already installed\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!mamba install -c conda-forge transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb6602-0a7d-409c-9474-9e6d3cc629a9",
   "metadata": {},
   "source": [
    "The Huggingface entry gives the following code to obtain the internal representations: (note you need to download the model first on `login-transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc5e764-eadb-4ad2-92d4-44704aa7ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "sequence_Example = \"A E T C Z A O\"\n",
    "sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f28cf-cf59-4c10-84ff-31791c802559",
   "metadata": {},
   "source": [
    "This is pretty simple. It first loads the model, replaces U, Z, O and B with X and then tokenizes the sequences. Then it runs the sequence and returns the encoded input.\n",
    "\n",
    "The ouput should be `protein x max_len x embedding_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db32a213-eb23-4144-abec-d8d6f6c4a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
      "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
      "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
      "         ...,\n",
      "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
      "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
      "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0d4035-973c-41ae-818c-0423a17cfde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
       "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
       "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
       "         ...,\n",
       "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
       "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
       "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_protbert_embeddings(sequence) -> torch.Tensor: \n",
    "    \n",
    "    try:\n",
    "        if tokenizer is not None or protbert_model is not None:\n",
    "            print('protBERT loaded')\n",
    "    except:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "        protbert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    \n",
    "    sequence = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "    encoded_input = tokenizer(sequence, return_tensors='pt')\n",
    "    output = protbert_model(**encoded_input)\n",
    "    \n",
    "    return output.last_hidden_state\n",
    "\n",
    "get_protbert_embeddings('\"A E T C Z A A E T C Z A A E T C Z A A E T C Z A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c45cc8-19a5-4a59-bdc3-137079cb390c",
   "metadata": {},
   "source": [
    "This is dumb, I didnt realize protBERT is only for protein level embeddings. Nevertheless, I'll leave it here.\n",
    "\n",
    "Now lets move on to protT5.\n",
    "\n",
    "## ProtT5\n",
    "do this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eee08f-0aa7-4541-86aa-a8c1d2a47bf8",
   "metadata": {},
   "source": [
    "## Built-in functions from graphein\n",
    "So it turns out that graphein has built-in functions for getting ESM embeddings at the residue level, and will immediately embed this into the graph as well. Lets see how it works.\n",
    "First we have to construct a graph from some protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c5b44f-f190-4206-8067-b41f2aa1e823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/23 13:56:34] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule                                         <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">embeddings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#45\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         graphein.protein.features.sequence.embeddings, you need to install:   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec cannot be installed via conda                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Alternatively, you can install graphein with the extras:              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pip install graphein<span style=\"font-weight: bold\">[</span>extras<span style=\"font-weight: bold\">]</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/23 13:56:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule                                         \u001b]8;id=816537;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py\u001b\\\u001b[2membeddings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=672331;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/sequence/embeddings.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         graphein.protein.features.sequence.embeddings, you need to install:   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec                                                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec cannot be installed via conda                                  \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Alternatively, you can install graphein with the extras:              \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pip install graphein\u001b[1m[\u001b[0mextras\u001b[1m]\u001b[0m                                          \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/16/23 13:56:38] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.visualisation, you  <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">visualisation.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         need to install: pytorch3d                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pytorch3d                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/16/23 13:56:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.visualisation, you  \u001b]8;id=116537;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py\u001b\\\u001b[2mvisualisation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=402733;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/visualisation.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         need to install: pytorch3d                                         \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d    \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pytorch3d                                                          \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.meshes, you need to        <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">meshes.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py#29\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         install: pytorch3d                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d pytorch3d <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.meshes, you need to        \u001b]8;id=598896;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py\u001b\\\u001b[2mmeshes.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=206285;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/meshes.py#29\u001b\\\u001b[2m29\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         install: pytorch3d                                                        \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d pytorch3d \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Deprotonating protein. This removes H atoms from the pdb_df dataframe    <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#186\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Deprotonating protein. This removes H atoms from the pdb_df dataframe    \u001b]8;id=205229;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=209060;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#186\u001b\\\u001b[2m186\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Detected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span> total nodes                                                 <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">graphs.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#438\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Detected \u001b[1;36m198\u001b[0m total nodes                                                 \u001b]8;id=323827;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py\u001b\\\u001b[2mgraphs.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=824711;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/graphs.py#438\u001b\\\u001b[2m438\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Reading meiler embeddings from:                                       <a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">amino_acid.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#57\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">3.10/site-packages/graphein/protein/features/nodes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meiler_embeddings.</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">csv</span>                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Reading meiler embeddings from:                                       \u001b]8;id=548390;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py\u001b\\\u001b[2mamino_acid.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=186124;file:///scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/graphein/protein/features/nodes/amino_acid.py#57\u001b\\\u001b[2m57\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python\u001b[0m \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m3.10/site-packages/graphein/protein/features/nodes/\u001b[0m\u001b[95mmeiler_embeddings.\u001b[0m \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[95mcsv\u001b[0m                                                                   \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/scicore/home/schwede/goetze0000/mambaforge/envs/hackathon/lib/python3.10/site-packages/rich/live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2b445ef7a8c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from graphein.protein.graphs import construct_graph\n",
    "\n",
    "config = ProteinGraphConfig()\n",
    "g = construct_graph(config=config, path=\"../structures/1a9m.pdb\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882524-6f27-43b4-8c7f-bdd1335f69c7",
   "metadata": {},
   "source": [
    "Now to obtain the ESM embeddings I use the `graphein.protein.features.sequence.embeddings.compute_esm_embedding` function. I probably need to download it again...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a537cd-b58a-4a89-993a-5578fa8c959e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphein\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m esm_residue_embedding\n\u001b[1;32m      3\u001b[0m g_m \u001b[38;5;241m=\u001b[39m esm_residue_embedding(g, model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mesm1b_t33_650M_UR50S\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mg_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from graphein.protein.features.sequence.embeddings import esm_residue_embedding\n",
    "\n",
    "g_m = esm_residue_embedding(g, model_name = 'esm1b_t33_650M_UR50S')\n",
    "\n",
    "g_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebafe914-cd12-48ce-bbff-00149f297538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chain_id': 'A',\n",
       " 'residue_name': 'PRO',\n",
       " 'residue_number': 1,\n",
       " 'atom_type': 'CA',\n",
       " 'element_symbol': 'C',\n",
       " 'coords': array([50.252,  6.592, 82.398]),\n",
       " 'b_factor': 52.33,\n",
       " 'meiler': dim_1    2.67\n",
       " dim_2    0.00\n",
       " dim_3    2.72\n",
       " dim_4    0.72\n",
       " dim_5    6.80\n",
       " dim_6    0.13\n",
       " dim_7    0.34\n",
       " Name: PRO, dtype: float64,\n",
       " 'esm_embedding': array([ 0.09015156, -0.02480064, -0.0604364 , ..., -0.17244197,\n",
       "        -0.04754237,  0.27853337], dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_m.nodes['A:PRO:1::']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c8a31-efe9-485d-afba-47f0bfbeb88d",
   "metadata": {},
   "source": [
    "So it appears that this function is broken! Excellent, that means my work does not go to waste. Now lets check how we can actually embed this stuff into the graph.\n",
    "\n",
    "looks like I need to make a partial function. Not a problem, lets give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39a1b3-56fa-43ce-a545-44dfcc14619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "config = ProteinGraphConfig(\n",
    "    graph_metadata_functions=[\n",
    "                             esm_residue_embedding,\n",
    "                             partial(esm_residue_embedding)\n",
    "                             ])\n",
    "g = construct_graph(config=config, path=\"../structures/1a9m.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbc840-5fe5-4293-8f09-ecbcf062e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph-level function that operates on sequences\n",
    "def fixed_esm_func(G: nx.Graph) -> nx.Graph:\n",
    "\n",
    "    # Define our feature function that operates on a sequence\n",
    "    def count_histidines(sequence: str) -> int:\n",
    "         return sequence.count(\"H\")\n",
    "\n",
    "    def get_esm_embedding(sequence: Union[str, list, tuple]) -> torch.Tensor:\n",
    "        global esm_model\n",
    "        global alphabet\n",
    "        global batch_converter\n",
    "\n",
    "        try: \n",
    "            if esm_model:\n",
    "                print('ESM is loaded')\n",
    "        except:\n",
    "            # Load the model if its not loaded\n",
    "            esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "            batch_converter = alphabet.get_batch_converter()\n",
    "            esm_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "\n",
    "        # Convert input to be [(name, sequence), ...]\n",
    "        # Can be a single string, list of strings, tuple, or list of tuples.\n",
    "        if type(sequence) == str:\n",
    "            data = [('protein0',sequence)]\n",
    "        elif type(sequence) == list:\n",
    "            data = [(f'protein{x}',sequence[x]) for x in range(len(sequence))]\n",
    "        elif type(sequence) == tuple and len(sequence) == 2:\n",
    "            data = [sequence]\n",
    "        elif type(sequence[0]) == tuple and len(sequence[0]) == 2:\n",
    "            data = sequence\n",
    "\n",
    "        # Obtain the batches\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "\n",
    "        return token_representations\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bbef81-f762-4f58-b491-2bd50eefceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm.pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4226444c-0d4e-40ef-94c8-9ce0a4fecd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ESM2(\n",
       "   (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "   (layers): ModuleList(\n",
       "     (0-32): 33 x TransformerLayer(\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         (rot_emb): RotaryEmbedding()\n",
       "       )\n",
       "       (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "       (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "       (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "       (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (contact_head): ContactPredictionHead(\n",
       "     (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "     (activation): Sigmoid()\n",
       "   )\n",
       "   (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "   (lm_head): RobertaLMHead(\n",
       "     (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "     (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       " ),\n",
       " <esm.data.Alphabet at 0x2b445ff77190>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import esm\n",
    "esm.pretrained.load_model_and_alphabet('esm2_t33_650M_UR50D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77eb38-b577-4c57-90e8-37c0a429fd97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mambaforge-hackathon]",
   "language": "python",
   "name": "conda-env-mambaforge-hackathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
